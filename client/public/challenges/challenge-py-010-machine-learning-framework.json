{
  "id": "py-machine-learning-framework",
  "metadata": {
    "title": "Python Mini Machine Learning Framework Implementation",
    "description": "Build a lightweight machine learning framework with neural networks, optimization, and training capabilities.\n\n## Learning Objectives\n- Neural network fundamentals\n- Backpropagation algorithm\n- Gradient descent optimization\n- Loss functions and activation functions",
    "difficulty": "hard",
    "points": 3,
    "timeLimit": 35,
    "tags": ["python", "machine-learning", "neural-networks", "numpy", "optimization"],
    "author": "Z-Challenge Team",
    "createdAt": "2025-10-05T12:20:00Z",
    "version": "1.0",
    "supportedLanguages": ["python"]
  },
  "problem": {
    "statement": "Create a mini machine learning framework supporting feedforward neural networks with customizable layers, activation functions, and optimization algorithms.",
    "inputFormat": "Network configuration and training data",
    "outputFormat": "Training progress and model predictions",
    "constraints": "- Implement from scratch without ML libraries\n- Support multiple activation functions\n- Implement backpropagation algorithm\n- Provide training and prediction capabilities",
    "examples": [
      {
        "input": "network = NeuralNetwork([2, 4, 1])\\ntrain_data = [[0,0],[0,1],[1,0],[1,1]]\\ntargets = [0,1,1,0]",
        "output": "Training completed. Final loss: 0.05\\nXOR predictions: [0.02, 0.98, 0.97, 0.03]",
        "explanation": "Train XOR function with neural network"
      }
    ]
  },
  "languages": {
    "python": {
      "starterCode": "import math\nimport random\nfrom typing import List, Callable, Tuple, Optional\nfrom dataclasses import dataclass\n\n# Activation functions\nclass ActivationFunction:\n    @staticmethod\n    def sigmoid(x: float) -> float:\n        # Your implementation here\n        return 0.0\n    \n    @staticmethod\n    def sigmoid_derivative(x: float) -> float:\n        # Your implementation here\n        return 0.0\n    \n    @staticmethod\n    def tanh(x: float) -> float:\n        # Your implementation here\n        return 0.0\n    \n    @staticmethod\n    def tanh_derivative(x: float) -> float:\n        # Your implementation here\n        return 0.0\n    \n    @staticmethod\n    def relu(x: float) -> float:\n        # Your implementation here\n        return 0.0\n    \n    @staticmethod\n    def relu_derivative(x: float) -> float:\n        # Your implementation here\n        return 0.0\n\nclass Matrix:\n    \"\"\"Simple matrix class for neural network operations\"\"\"\n    \n    def __init__(self, rows: int, cols: int, data: List[List[float]] = None):\n        self.rows = rows\n        self.cols = cols\n        if data:\n            self.data = data\n        else:\n            self.data = [[0.0 for _ in range(cols)] for _ in range(rows)]\n    \n    def randomize(self, min_val: float = -1.0, max_val: float = 1.0):\n        # Your implementation here\n        pass\n    \n    def multiply(self, other: 'Matrix') -> 'Matrix':\n        # Matrix multiplication\n        # Your implementation here\n        return Matrix(1, 1)\n    \n    def add(self, other: 'Matrix') -> 'Matrix':\n        # Element-wise addition\n        # Your implementation here\n        return Matrix(1, 1)\n    \n    def subtract(self, other: 'Matrix') -> 'Matrix':\n        # Element-wise subtraction\n        # Your implementation here\n        return Matrix(1, 1)\n    \n    def transpose(self) -> 'Matrix':\n        # Matrix transpose\n        # Your implementation here\n        return Matrix(1, 1)\n    \n    def apply_function(self, func: Callable[[float], float]) -> 'Matrix':\n        # Apply function to each element\n        # Your implementation here\n        return Matrix(1, 1)\n    \n    def to_array(self) -> List[float]:\n        # Convert to 1D array (for single column matrices)\n        return [self.data[i][0] for i in range(self.rows)]\n\nclass Layer:\n    \"\"\"Neural network layer\"\"\"\n    \n    def __init__(self, input_size: int, output_size: int, activation: str = 'sigmoid'):\n        self.input_size = input_size\n        self.output_size = output_size\n        self.activation = activation\n        \n        # Initialize weights and biases\n        self.weights = Matrix(output_size, input_size)\n        self.weights.randomize(-1.0, 1.0)\n        self.biases = Matrix(output_size, 1)\n        self.biases.randomize(-1.0, 1.0)\n        \n        # For backpropagation\n        self.last_input = None\n        self.last_output = None\n        \n    def forward(self, inputs: Matrix) -> Matrix:\n        # Forward propagation\n        # Your implementation here\n        return Matrix(1, 1)\n    \n    def get_activation_function(self):\n        # Return activation function and its derivative\n        # Your implementation here\n        return ActivationFunction.sigmoid, ActivationFunction.sigmoid_derivative\n\nclass NeuralNetwork:\n    \"\"\"Feedforward neural network\"\"\"\n    \n    def __init__(self, layer_sizes: List[int], activations: List[str] = None):\n        self.layers = []\n        \n        # Create layers\n        for i in range(len(layer_sizes) - 1):\n            activation = 'sigmoid'\n            if activations and i < len(activations):\n                activation = activations[i]\n            \n            layer = Layer(layer_sizes[i], layer_sizes[i + 1], activation)\n            self.layers.append(layer)\n    \n    def forward(self, inputs: List[float]) -> List[float]:\n        \"\"\"Forward propagation through the network\"\"\"\n        # Your implementation here\n        return [0.0]\n    \n    def train(self, training_data: List[List[float]], targets: List[List[float]], \n             epochs: int = 1000, learning_rate: float = 0.1) -> List[float]:\n        \"\"\"Train the network using backpropagation\"\"\"\n        # Your implementation here\n        return [0.0]\n    \n    def backward(self, inputs: List[float], targets: List[float], learning_rate: float):\n        \"\"\"Backpropagation algorithm\"\"\"\n        # Your implementation here\n        pass\n    \n    def calculate_loss(self, predictions: List[float], targets: List[float]) -> float:\n        \"\"\"Calculate mean squared error loss\"\"\"\n        # Your implementation here\n        return 0.0\n\n# Test the neural network\nif __name__ == \"__main__\":\n    # XOR problem\n    print(\"Training XOR function...\")\n    \n    # Training data\n    training_inputs = [\n        [0, 0],\n        [0, 1],\n        [1, 0],\n        [1, 1]\n    ]\n    \n    training_targets = [\n        [0],\n        [1],\n        [1],\n        [0]\n    ]\n    \n    # Create network: 2 inputs, 4 hidden neurons, 1 output\n    network = NeuralNetwork([2, 4, 1])\n    \n    # Train the network\n    losses = network.train(training_inputs, training_targets, epochs=5000, learning_rate=0.5)\n    \n    print(f\"Final loss: {losses[-1]:.4f}\")\n    \n    # Test the network\n    print(\"\\nTesting XOR function:\")\n    for i, inputs in enumerate(training_inputs):\n        prediction = network.forward(inputs)\n        target = training_targets[i][0]\n        print(f\"Input: {inputs}, Target: {target}, Prediction: {prediction[0]:.3f}\")\n    \n    # Test with simple linear problem\n    print(\"\\n\" + \"=\"*50)\n    print(\"Training simple linear function (y = 2x + 1)...\")\n    \n    # Generate training data for y = 2x + 1\n    linear_inputs = [[x/10.0] for x in range(-50, 51, 5)]\n    linear_targets = [[2 * x[0] + 1] for x in linear_inputs]\n    \n    # Create simpler network for linear problem\n    linear_network = NeuralNetwork([1, 3, 1])\n    \n    # Train\n    linear_losses = linear_network.train(linear_inputs, linear_targets, epochs=2000, learning_rate=0.01)\n    \n    print(f\"Final loss: {linear_losses[-1]:.6f}\")\n    \n    # Test some values\n    test_values = [0.0, 1.0, 2.0, -1.0, 0.5]\n    print(\"\\nTesting linear function:\")\n    for x in test_values:\n        prediction = linear_network.forward([x])\n        target = 2 * x + 1\n        print(f\"Input: {x}, Target: {target}, Prediction: {prediction[0]:.3f}\")",
      "solutionCode": "import math\nimport random\nfrom typing import List, Callable, Tuple, Optional\nfrom dataclasses import dataclass\n\n# Activation functions\nclass ActivationFunction:\n    @staticmethod\n    def sigmoid(x: float) -> float:\n        try:\n            return 1.0 / (1.0 + math.exp(-x))\n        except OverflowError:\n            return 0.0 if x < 0 else 1.0\n    \n    @staticmethod\n    def sigmoid_derivative(x: float) -> float:\n        s = ActivationFunction.sigmoid(x)\n        return s * (1 - s)\n    \n    @staticmethod\n    def tanh(x: float) -> float:\n        return math.tanh(x)\n    \n    @staticmethod\n    def tanh_derivative(x: float) -> float:\n        t = math.tanh(x)\n        return 1 - t * t\n    \n    @staticmethod\n    def relu(x: float) -> float:\n        return max(0.0, x)\n    \n    @staticmethod\n    def relu_derivative(x: float) -> float:\n        return 1.0 if x > 0 else 0.0\n\nclass Matrix:\n    \"\"\"Simple matrix class for neural network operations\"\"\"\n    \n    def __init__(self, rows: int, cols: int, data: List[List[float]] = None):\n        self.rows = rows\n        self.cols = cols\n        if data:\n            self.data = [row[:] for row in data]  # Deep copy\n        else:\n            self.data = [[0.0 for _ in range(cols)] for _ in range(rows)]\n    \n    def randomize(self, min_val: float = -1.0, max_val: float = 1.0):\n        for i in range(self.rows):\n            for j in range(self.cols):\n                self.data[i][j] = random.uniform(min_val, max_val)\n    \n    def multiply(self, other: 'Matrix') -> 'Matrix':\n        \"\"\"Matrix multiplication\"\"\"\n        if self.cols != other.rows:\n            raise ValueError(f\"Cannot multiply {self.rows}x{self.cols} with {other.rows}x{other.cols}\")\n        \n        result = Matrix(self.rows, other.cols)\n        for i in range(self.rows):\n            for j in range(other.cols):\n                for k in range(self.cols):\n                    result.data[i][j] += self.data[i][k] * other.data[k][j]\n        \n        return result\n    \n    def add(self, other: 'Matrix') -> 'Matrix':\n        \"\"\"Element-wise addition\"\"\"\n        if self.rows != other.rows or self.cols != other.cols:\n            raise ValueError(\"Matrix dimensions must match for addition\")\n        \n        result = Matrix(self.rows, self.cols)\n        for i in range(self.rows):\n            for j in range(self.cols):\n                result.data[i][j] = self.data[i][j] + other.data[i][j]\n        \n        return result\n    \n    def subtract(self, other: 'Matrix') -> 'Matrix':\n        \"\"\"Element-wise subtraction\"\"\"\n        if self.rows != other.rows or self.cols != other.cols:\n            raise ValueError(\"Matrix dimensions must match for subtraction\")\n        \n        result = Matrix(self.rows, self.cols)\n        for i in range(self.rows):\n            for j in range(self.cols):\n                result.data[i][j] = self.data[i][j] - other.data[i][j]\n        \n        return result\n    \n    def transpose(self) -> 'Matrix':\n        \"\"\"Matrix transpose\"\"\"\n        result = Matrix(self.cols, self.rows)\n        for i in range(self.rows):\n            for j in range(self.cols):\n                result.data[j][i] = self.data[i][j]\n        \n        return result\n    \n    def apply_function(self, func: Callable[[float], float]) -> 'Matrix':\n        \"\"\"Apply function to each element\"\"\"\n        result = Matrix(self.rows, self.cols)\n        for i in range(self.rows):\n            for j in range(self.cols):\n                result.data[i][j] = func(self.data[i][j])\n        \n        return result\n    \n    def scale(self, scalar: float) -> 'Matrix':\n        \"\"\"Multiply all elements by scalar\"\"\"\n        result = Matrix(self.rows, self.cols)\n        for i in range(self.rows):\n            for j in range(self.cols):\n                result.data[i][j] = self.data[i][j] * scalar\n        \n        return result\n    \n    def hadamard(self, other: 'Matrix') -> 'Matrix':\n        \"\"\"Element-wise multiplication (Hadamard product)\"\"\"\n        if self.rows != other.rows or self.cols != other.cols:\n            raise ValueError(\"Matrix dimensions must match for Hadamard product\")\n        \n        result = Matrix(self.rows, self.cols)\n        for i in range(self.rows):\n            for j in range(self.cols):\n                result.data[i][j] = self.data[i][j] * other.data[i][j]\n        \n        return result\n    \n    def to_array(self) -> List[float]:\n        \"\"\"Convert to 1D array (for single column matrices)\"\"\"\n        if self.cols != 1:\n            raise ValueError(\"Can only convert single column matrix to array\")\n        return [self.data[i][0] for i in range(self.rows)]\n    \n    @staticmethod\n    def from_array(arr: List[float]) -> 'Matrix':\n        \"\"\"Create column matrix from array\"\"\"\n        result = Matrix(len(arr), 1)\n        for i in range(len(arr)):\n            result.data[i][0] = arr[i]\n        return result\n\nclass Layer:\n    \"\"\"Neural network layer\"\"\"\n    \n    def __init__(self, input_size: int, output_size: int, activation: str = 'sigmoid'):\n        self.input_size = input_size\n        self.output_size = output_size\n        self.activation = activation\n        \n        # Initialize weights and biases with Xavier initialization\n        self.weights = Matrix(output_size, input_size)\n        xavier_std = math.sqrt(2.0 / (input_size + output_size))\n        self.weights.randomize(-xavier_std, xavier_std)\n        \n        self.biases = Matrix(output_size, 1)\n        self.biases.randomize(-0.5, 0.5)\n        \n        # For backpropagation\n        self.last_input = None\n        self.last_output = None\n        self.last_weighted_sum = None\n        \n    def forward(self, inputs: Matrix) -> Matrix:\n        \"\"\"Forward propagation\"\"\"\n        self.last_input = inputs\n        \n        # Calculate weighted sum: W * x + b\n        weighted_sum = self.weights.multiply(inputs).add(self.biases)\n        self.last_weighted_sum = weighted_sum\n        \n        # Apply activation function\n        activation_func, _ = self.get_activation_function()\n        output = weighted_sum.apply_function(activation_func)\n        \n        self.last_output = output\n        return output\n    \n    def get_activation_function(self):\n        \"\"\"Return activation function and its derivative\"\"\"\n        if self.activation == 'sigmoid':\n            return ActivationFunction.sigmoid, ActivationFunction.sigmoid_derivative\n        elif self.activation == 'tanh':\n            return ActivationFunction.tanh, ActivationFunction.tanh_derivative\n        elif self.activation == 'relu':\n            return ActivationFunction.relu, ActivationFunction.relu_derivative\n        else:\n            return ActivationFunction.sigmoid, ActivationFunction.sigmoid_derivative\n\nclass NeuralNetwork:\n    \"\"\"Feedforward neural network\"\"\"\n    \n    def __init__(self, layer_sizes: List[int], activations: List[str] = None):\n        self.layers = []\n        \n        # Create layers\n        for i in range(len(layer_sizes) - 1):\n            activation = 'sigmoid'\n            if activations and i < len(activations):\n                activation = activations[i]\n            \n            layer = Layer(layer_sizes[i], layer_sizes[i + 1], activation)\n            self.layers.append(layer)\n    \n    def forward(self, inputs: List[float]) -> List[float]:\n        \"\"\"Forward propagation through the network\"\"\"\n        current_input = Matrix.from_array(inputs)\n        \n        for layer in self.layers:\n            current_input = layer.forward(current_input)\n        \n        return current_input.to_array()\n    \n    def train(self, training_data: List[List[float]], targets: List[List[float]], \n             epochs: int = 1000, learning_rate: float = 0.1) -> List[float]:\n        \"\"\"Train the network using backpropagation\"\"\"\n        losses = []\n        \n        for epoch in range(epochs):\n            total_loss = 0.0\n            \n            # Shuffle training data\n            combined = list(zip(training_data, targets))\n            random.shuffle(combined)\n            \n            for inputs, target in combined:\n                # Forward pass\n                prediction = self.forward(inputs)\n                \n                # Calculate loss\n                loss = self.calculate_loss(prediction, target)\n                total_loss += loss\n                \n                # Backward pass\n                self.backward(inputs, target, learning_rate)\n            \n            avg_loss = total_loss / len(training_data)\n            losses.append(avg_loss)\n            \n            # Print progress occasionally\n            if epoch % (epochs // 10) == 0 or epoch == epochs - 1:\n                print(f\"Epoch {epoch}, Loss: {avg_loss:.6f}\")\n        \n        return losses\n    \n    def backward(self, inputs: List[float], targets: List[float], learning_rate: float):\n        \"\"\"Backpropagation algorithm\"\"\"\n        # Convert to matrices\n        input_matrix = Matrix.from_array(inputs)\n        target_matrix = Matrix.from_array(targets)\n        \n        # Forward pass to populate layer states\n        self.forward(inputs)\n        \n        # Calculate output layer error\n        output_layer = self.layers[-1]\n        output_error = output_layer.last_output.subtract(target_matrix)\n        \n        # Backpropagate errors\n        current_error = output_error\n        \n        for i in reversed(range(len(self.layers))):\n            layer = self.layers[i]\n            \n            # Get activation derivative\n            _, activation_derivative = layer.get_activation_function()\n            \n            # Calculate gradients\n            activation_gradients = layer.last_weighted_sum.apply_function(activation_derivative)\n            gradients = current_error.hadamard(activation_gradients)\n            \n            # Calculate weight and bias updates\n            weight_gradients = gradients.multiply(layer.last_input.transpose())\n            \n            # Update weights and biases\n            layer.weights = layer.weights.subtract(weight_gradients.scale(learning_rate))\n            layer.biases = layer.biases.subtract(gradients.scale(learning_rate))\n            \n            # Calculate error for previous layer\n            if i > 0:\n                current_error = layer.weights.transpose().multiply(gradients)\n    \n    def calculate_loss(self, predictions: List[float], targets: List[float]) -> float:\n        \"\"\"Calculate mean squared error loss\"\"\"\n        total_error = 0.0\n        for i in range(len(predictions)):\n            error = predictions[i] - targets[i]\n            total_error += error * error\n        \n        return total_error / len(predictions)\n\n# Test the neural network\nif __name__ == \"__main__\":\n    # Set random seed for reproducibility\n    random.seed(42)\n    \n    # XOR problem\n    print(\"Training XOR function...\")\n    \n    # Training data\n    training_inputs = [\n        [0, 0],\n        [0, 1],\n        [1, 0],\n        [1, 1]\n    ]\n    \n    training_targets = [\n        [0],\n        [1],\n        [1],\n        [0]\n    ]\n    \n    # Create network: 2 inputs, 4 hidden neurons, 1 output\n    network = NeuralNetwork([2, 4, 1])\n    \n    # Train the network\n    losses = network.train(training_inputs, training_targets, epochs=5000, learning_rate=0.5)\n    \n    print(f\"Final loss: {losses[-1]:.4f}\")\n    \n    # Test the network\n    print(\"\\nTesting XOR function:\")\n    for i, inputs in enumerate(training_inputs):\n        prediction = network.forward(inputs)\n        target = training_targets[i][0]\n        print(f\"Input: {inputs}, Target: {target}, Prediction: {prediction[0]:.3f}\")\n    \n    # Test with simple linear problem\n    print(\"\\n\" + \"=\"*50)\n    print(\"Training simple linear function (y = 2x + 1)...\")\n    \n    # Generate training data for y = 2x + 1\n    linear_inputs = [[x/10.0] for x in range(-50, 51, 5)]\n    linear_targets = [[2 * x[0] + 1] for x in linear_inputs]\n    \n    # Create simpler network for linear problem\n    linear_network = NeuralNetwork([1, 3, 1])\n    \n    # Train\n    linear_losses = linear_network.train(linear_inputs, linear_targets, epochs=2000, learning_rate=0.01)\n    \n    print(f\"Final loss: {linear_losses[-1]:.6f}\")\n    \n    # Test some values\n    test_values = [0.0, 1.0, 2.0, -1.0, 0.5]\n    print(\"\\nTesting linear function:\")\n    for x in test_values:\n        prediction = linear_network.forward([x])\n        target = 2 * x + 1\n        print(f\"Input: {x}, Target: {target}, Prediction: {prediction[0]:.3f}\")",
      "hints": [
        "Use Xavier initialization for better weight initialization",
        "Implement matrix operations carefully for backpropagation",
        "Handle numerical overflow in sigmoid function",
        "Use Hadamard product for element-wise multiplication in gradients"
      ],
      "judge0Id": 71,
      "compilerType": "judge0"
    }
  },
  "testCases": [
    {
      "id": "test-xor-training",
      "input": "",
      "output": "Training XOR function...\\nEpoch 0, Loss: \\nEpoch 500, Loss: \\nEpoch 1000, Loss: \\nEpoch 1500, Loss: \\nEpoch 2000, Loss: \\nEpoch 2500, Loss: \\nEpoch 3000, Loss: \\nEpoch 3500, Loss: \\nEpoch 4000, Loss: \\nEpoch 4500, Loss: \\nEpoch 4999, Loss: \\nFinal loss: \\n\\nTesting XOR function:\\nInput: [0, 0], Target: 0, Prediction: \\nInput: [0, 1], Target: 1, Prediction: \\nInput: [1, 0], Target: 1, Prediction: \\nInput: [1, 1], Target: 0, Prediction:",
      "points": 6,
      "isHidden": false,
      "timeout": 15000
    },
    {
      "id": "test-linear-function",
      "input": "",
      "output": "Training simple linear function (y = 2x + 1)...\\nEpoch 0, Loss: \\nEpoch 200, Loss: \\nEpoch 400, Loss: \\nEpoch 600, Loss: \\nEpoch 800, Loss: \\nEpoch 1000, Loss: \\nEpoch 1200, Loss: \\nEpoch 1400, Loss: \\nEpoch 1600, Loss: \\nEpoch 1800, Loss: \\nEpoch 1999, Loss: \\nFinal loss:",
      "points": 4,
      "isHidden": true,
      "timeout": 10000
    }
  ],
  "editorial": {
    "approach": "A mini machine learning framework implements core neural network concepts from scratch, providing deep understanding of backpropagation, gradient descent, and matrix operations in deep learning.",
    "complexity": {
      "time": "O(n * m * k) per epoch where n=samples, m=layers, k=neurons per layer",
      "space": "O(m * kÂ²) for storing weights and intermediate values"
    },
    "keyPoints": [
      "Matrix operations for efficient neural network computation",
      "Backpropagation algorithm for gradient calculation",
      "Xavier initialization for stable training",
      "Multiple activation functions and their derivatives"
    ]
  }
}